{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaigning Video Generations\n",
    "\n",
    "This notebook will help you quickly start generating longer form videos using Amazon Nova Reel by stitching together multiple 6 second generations.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Be sure you've followed the instructions in the [00_initial_setup.ipynb](../00_initial_setup.ipynb) notebook to get things set up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure credentials and shared parameters\n",
    "\n",
    "Run the cell below to set the default session configuration for all AWS SDK calls made by the other cells in this notebook. As written, the code will default to using the user credentials you have set as your \"default\" via the AWS CLI. If you'd like to use different credentials, you can modify the code below to add `aws_access_key_id` and `aws_secret_access_key` arguments to the setup function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_ID = \"amazon.nova-reel-v1:0\"\n",
    "\n",
    "# Set default region and credentials.\n",
    "boto3.setup_default_session(\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "logger.info(\"AWS SDK session defaults have been set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the model\n",
    "\n",
    "Generating a video takes some time - approximately 3.5 minutes to produce a 6 second video. To accomodate this execution time, the Bedrock Runtime introduces a new asynchronous invocation API. Calling `start_async_invoke()` creates a new invocation job. When the job completes, Bedrock automatically saves the generated video to an S3 bucket you specify.\n",
    "\n",
    "### Image-to-Video\n",
    "\n",
    "You can also generate videos by providing an initial starting image and a text prompt. For best results, the text prompt should describe the image and also provide details about the desired action and camera movement you'd like the video to have. Modify the `s3_destination_bucket`, `input_image_path`, and `video_prompt` variables at the start of the code below and then run the cell to start generating your video.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <div style=\"width: 31%\">\n",
    "        <p align=\"center\">\n",
    "            <video alt=\"example_text_to_video\" controls style=\"padding: 4px\" >\n",
    "                <source src=\"../videos/snow_1.mp4\" type=\"video/mp4\" >\n",
    "            </video>\n",
    "            <br>\n",
    "            <em>First video</em>\n",
    "        </p>\n",
    "    </div>\n",
    "    <div style=\"width: 31%;\">\n",
    "        <p align=\"center\">\n",
    "            <img src=\"../images/snow_last_frame.jpg\" width=\"100%\" style=\"padding: 4px\">\n",
    "            <br>\n",
    "            <em>Last frame image</em>\n",
    "        </p>\n",
    "    </div>\n",
    "    <div style=\"width: 31%\">\n",
    "        <p align=\"center\">\n",
    "            <video alt=\"example_text_to_video\" controls style=\"padding: 4px\" >\n",
    "                <source src=\"../videos/snow_merged.mp4\" type=\"video/mp4\" >\n",
    "            </video>\n",
    "            <br>\n",
    "            <em>Merged video</em>\n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import amazon_video_util\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT: Modify the S3 destination (s3_destination_bucket) and video prompt (video_prompt) below.\n",
    "\"\"\"\n",
    "\n",
    "# Specify an S3 bucket for the video output.\n",
    "s3_destination_bucket = \"nova-videos\"  # Change this to a unique bucket name.\n",
    "\n",
    "# Specify your video generation prompt. Phrase your prompt as a summary rather than a command. Maximum 512 characters.\n",
    "video_prompt = \"First person view walking through light snowfall in a forest, sunlight through trees, dolly forward, cinematic\"\n",
    "\n",
    "\"\"\"\n",
    "STOP: You should not have to modify anything below this line.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the S3 client.\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Create the S3 bucket.\n",
    "s3_client.create_bucket(Bucket=s3_destination_bucket)\n",
    "\n",
    "# Create the Bedrock Runtime client.\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "model_input = {\n",
    "    \"taskType\": \"TEXT_VIDEO\",\n",
    "    \"textToVideoParams\": {\n",
    "        \"text\": video_prompt\n",
    "        },\n",
    "    \"videoGenerationConfig\": {\n",
    "        \"durationSeconds\": 6, \n",
    "        \"fps\": 24,\n",
    "        \"dimension\": \"1280x720\",\n",
    "        \"seed\": random.randint(0, 2147483646),\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Start the asynchronous video generation job.\n",
    "    invocation = bedrock_runtime.start_async_invoke(\n",
    "        modelId=\"amazon.nova-reel-v1:0\",\n",
    "        modelInput=model_input,\n",
    "        outputDataConfig={\"s3OutputDataConfig\": {\"s3Uri\": f\"s3://{s3_destination_bucket}\"}},\n",
    "    )\n",
    "\n",
    "    # This will be used by other cells in this notebook.\n",
    "    invocation_arn = invocation[\"invocationArn\"]\n",
    "\n",
    "    # Pretty print the response JSON.\n",
    "    logger.info(\"\\nResponse:\")\n",
    "    logger.info(json.dumps(invocation, indent=2, default=str))\n",
    "\n",
    "    # Save the invocation details for later reference. Helpful for debugging and reporting feedback.\n",
    "    amazon_video_util.save_invocation_info(invocation, model_input)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the status of generation jobs\n",
    "\n",
    "We've provided a set of utility functions in the `amazon_video_util.py` script. One of these functions will automatically download a job if it is completed or monitor it while it is in-progress. The `invocation_arn` is defined in the code cell above and passed in below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor and download the first video generation\n",
    "local_file_path_1 = amazon_video_util.monitor_and_download_video(invocation_arn, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Frame Extraction\n",
    "\n",
    "Below we will first extract the last frame from the first video to use as a starting point for the second videos generation. This will allow for extending the original video past the 6 second limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "# Define and create an output directory with a unique name.\n",
    "generation_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_directory = f\"./output/{generation_id}\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Extract the last frame from the video\n",
    "output_path = f\"{output_directory}/last_frame.png\"\n",
    "amazon_video_util.extract_last_frame(local_file_path_1, output_path)\n",
    "\n",
    "# Display the last frame\n",
    "with Image.open(output_path) as last_frame:\n",
    "    last_frame.load()\n",
    "    display(last_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the second video\n",
    "Using the frame above we can now extend the first video. Make sure the prompt maintains the core elements and objects from the first generation to get the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT: Modify the video prompt (video_prompt) below.\n",
    "\"\"\"\n",
    "\n",
    "# Specify your video generation prompt. Phrase your prompt as a summary rather than a command. Maximum 512 characters.\n",
    "video_prompt = \"First person view entering a clearing with heavy snowfall, sun creating diamond-like sparkles, continuing dolly forward, cinematic\"\n",
    "\n",
    "\"\"\"\n",
    "STOP: You should not have to modify anything below this line.\n",
    "\"\"\"\n",
    "\n",
    "# Load the input image as a Base64 string.\n",
    "buffered = BytesIO()\n",
    "last_frame.save(buffered, format=\"JPEG\")\n",
    "input_image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "model_input = {\n",
    "    \"taskType\": \"TEXT_VIDEO\",\n",
    "    \"textToVideoParams\": {\n",
    "        \"text\": video_prompt,\n",
    "        \"images\": [\n",
    "            {\n",
    "                \"format\": \"png\",  # May be \"png\" or \"jpeg\"\n",
    "                \"source\": {\"bytes\": input_image_base64},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    \"videoGenerationConfig\": {\n",
    "        \"durationSeconds\": 6,  # 6 is the only supported value currently.\n",
    "        \"fps\": 24,  # 24 is the only supported value currently.\n",
    "        \"dimension\": \"1280x720\",  # \"1280x720\" is the only supported value currently.\n",
    "        \"seed\": random.randint(\n",
    "            0, 2147483648\n",
    "        ),  # A random seed guarantees we'll get a different result each time this code runs.\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Start the asynchronous video generation job.\n",
    "    invocation = bedrock_runtime.start_async_invoke(\n",
    "        modelId=\"amazon.nova-reel-v1:0\",\n",
    "        modelInput=model_input,\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\"s3Uri\": f\"s3://{s3_destination_bucket}\"}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # This will be used by other cells in this notebook.\n",
    "    invocation_arn = invocation[\"invocationArn\"]\n",
    "\n",
    "    # Pretty print the response JSON.\n",
    "    logger.info(\"\\nResponse:\")\n",
    "    logger.info(json.dumps(invocation, indent=2, default=str))\n",
    "\n",
    "    # Save the invocation details for later reference. Helpful for debugging and reporting feedback.\n",
    "    amazon_video_util.save_invocation_info(invocation, model_input)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the second generation\n",
    "As with the first generation, we will monitor and download the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_path_2 = amazon_video_util.monitor_and_download_video(invocation_arn, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch the videos\n",
    "We've provided another utility function in `amazon_video_util` that can be used to stitch the two videos together into a single video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output path for merged video\n",
    "output_path = f\"{output_directory}/merged_video.mp4\"\n",
    "\n",
    "# Stitch the two video generations together\n",
    "amazon_video_util.stitch_videos(local_file_path_1, local_file_path_2, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing final video\n",
    "Finally we can now view our extended video. You can try varying the seed and prompt of the second generation to get the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(output_path, embed=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
