{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10a6128-2ef7-4d88-94b3-f2ec5ca11d01",
   "metadata": {},
   "source": [
    "# Product Image and Video Generation with Amazon Nova and Amazon Bedrock: Background Replacement, Motion, and Overlay Techniques\n",
    "\n",
    "This notebook will help you quickly start generating your own videos from your product images using **Amazon Nova Canvas** and **Amazon Nova Reel** on **Amazon Bedrock**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f22fa-a3c1-418f-9426-bc7029d0d1cd",
   "metadata": {},
   "source": [
    "\n",
    "## Detailed Step-by-Step Workflow\n",
    "\n",
    "### Step 1: Background Replacement with Mask Prompt\n",
    "- **Objective**: Prepare and transform the original product image\n",
    "- **Key Actions**:\n",
    "  - Resize and pad the input image to meet 1280x720 video specifications\n",
    "  - Use Nova Canvas for outpainting\n",
    "  - Apply a mask prompt to preserve the product\n",
    "  - Generate a new, contextually rich background\n",
    "\n",
    "### Step 2: Video Generation with Nova Reel\n",
    "- **Objective**: Create a dynamic video with controlled motion\n",
    "- **Key Actions**:\n",
    "  - Use the image generated in Step 1 as the initial frame\n",
    "  - Generate a 6-second video with background movement\n",
    "  - Craft a precise text prompt for controlled motion\n",
    "\n",
    "### Step 3: Background Removal\n",
    "- **Objective**: Isolate the product with transparency\n",
    "- **Key Actions**:\n",
    "  - Remove the background from the image generated in Step 1\n",
    "  - Prepare for frame overlay\n",
    "\n",
    "### Step 4: Product Overlay and Final Video Composition\n",
    "- **Objective**: Preserve brand identity and product details\n",
    "- **Key Actions**:\n",
    "  - Split generated video into individual frames\n",
    "  - Overlay original product image (image generated in Step 3) on each frame\n",
    "  - Reconstruct video with preserved product details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d007dd-bf87-4e78-be34-e756b6d06db9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 1: Background replacement using mask prompt\n",
    "\n",
    "One way to replace the background in an image is to use the \"OUTPAINTING\" task type with a `maskPrompt` describing the object(s) to keep and a text prompt describing the new background. A common use case for this feature is placing a real product in a generated environment.\n",
    "\n",
    "The script below will use the image on the left as input. It will replace the background behind the product by using the `maskPrompt` prompt *\"product\"* and the `text` prompt *\"Product in a serene outdoor garden setting, soft morning light filtered through leaves. The product is placed in the middle of a white stone or marble platform, surrounded by lavender plants in soft focus. Fresh dewdrops on foliage catching sunlight. Delicate morning mist in background. Depth of field emphasizing product in foreground. Natural color palette with soft purples, greens, and white tones. Professional product photography, 8K resolution. Golden hour lighting creating gentle rim light on bottle edge and generating a shadow of the product. Hyper-realistic botanical details with macro-style flower elements. Clean, fresh atmosphere matching product aesthetic.\"*. \n",
    "\n",
    "The result will look similar to the image on the right. Note the realistic shadows and reflections that ground the original product into the generated scene.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "  <div>\n",
    "    <img src=\"./example-images/baby_oil.png\" style=\"max-height: 300px; max-width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Input image</p>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img src=\"example-images/background-replacement-baby_oil.png\" style=\"max-height: 300px; max-width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Output image</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Edit the `text` prompt to experiment with putting the product in different settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a5d610-46ec-4e7a-8bc9-81151d6bf76a",
   "metadata": {},
   "source": [
    "Let's install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85149f28-3701-4cb0-9545-b5d5cfa37bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install moviepy \n",
    "%pip install --upgrade boto3\n",
    "%pip install --upgrade ipython opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee6c4b-9e12-45a0-9d01-f2b75ab49fc7",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30849ac-2297-4a64-b2b4-188fc57c2a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import logging\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d56598-29b0-42a5-898d-37208d4b296b",
   "metadata": {},
   "source": [
    "Let's import some utils first that will help us with the image generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122686e6-968d-42a8-9eac-35efebed1dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../image-generation/python')\n",
    "from amazon_image_gen import BedrockImageGenerator\n",
    "import file_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16ce8b-b92c-4f4b-b1e5-be7192316b54",
   "metadata": {},
   "source": [
    "The following utility function transforms input images to a consistent 1280x720 pixel format while preserving the original aspect ratio. It resizes the image, maintaining its proportions by adding white padding either vertically or horizontally. \n",
    "\n",
    "The function is able to pad in three different `pad_modes: left, center and right`. `left` will position the product at the left and pad at the right, `center` will leave the product at the center of the image and pad both to the left and right and finally, `right` will position the product at the right and add pad to the left.\n",
    "\n",
    "An additional argument `resize_percent`can be provided in the funtion to reduce the size of the product to fit better on the new scene\n",
    "\n",
    "The result is a standardized image prepared for video generation, ensuring no distortion occurs during the transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b9dac3-6e83-4254-ac3b-7c20ae081ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image_path, target_width=1280, target_height=720, horizontal_placement=50, vertical_placement=50, resize_percent=50):\n",
    "    \"\"\"\n",
    "    Resize and pad an image to fit the target dimensions, allowing custom horizontal and vertical placement percentages.\n",
    "    The placement percentages use the center of the resized image as reference point.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        target_width (int): Target width of the output image.\n",
    "        target_height (int): Target height of the output image.\n",
    "        horizontal_placement (int): Horizontal placement percentage (0 to 100).\n",
    "        vertical_placement (int): Vertical placement percentage (0 to 100).\n",
    "        resize_percent (int): Percentage to resize the image.\n",
    "\n",
    "    Returns:\n",
    "        str: Base64-encoded string of the final image.\n",
    "    \"\"\"\n",
    "    # Validate input percentages\n",
    "    if not (0 <= horizontal_placement <= 100):\n",
    "        raise ValueError(\"horizontal_placement must be between 0 and 100\")\n",
    "    if not (0 <= vertical_placement <= 100):\n",
    "        raise ValueError(\"vertical_placement must be between 0 and 100\")\n",
    "    if not (0 <= resize_percent <= 100):\n",
    "        raise ValueError(\"resize_percent must be between 0 and 100\")\n",
    "\n",
    "    # Read image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert to RGB if needed\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Calculate aspect ratios\n",
    "    target_aspect = target_width / target_height\n",
    "    img_aspect = img.width / img.height\n",
    "\n",
    "    if img_aspect > target_aspect:\n",
    "        new_width = int(target_width * (resize_percent / 100))\n",
    "        new_height = int(new_width / img_aspect)\n",
    "    else:\n",
    "        new_height = int(target_height * (resize_percent / 100))\n",
    "        new_width = int(new_height * img_aspect)\n",
    "\n",
    "    # Resize image\n",
    "    img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Calculate padding based on placement percentages, using the center of the resized image as reference\n",
    "    available_width = target_width - new_width\n",
    "    available_height = target_height - new_height\n",
    "    \n",
    "    pad_left = int(available_width * (horizontal_placement / 100))\n",
    "    pad_top = int(available_height * (vertical_placement / 100))\n",
    "\n",
    "    # Create new white image with target size\n",
    "    final_img = Image.new('RGB', (target_width, target_height), 'white')\n",
    "\n",
    "    # Paste resized image onto white background\n",
    "    final_img.paste(img_resized, (pad_left, pad_top))\n",
    "\n",
    "    # Convert to base64\n",
    "    buffered = io.BytesIO()\n",
    "    final_img.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194df49-2044-4643-9108-a4206848eef0",
   "metadata": {},
   "source": [
    "The next code generates a new background for the provided product image using Nova Canvas image generation capabilities. It configures outpainting parameters using `\"outPaintingMode\": \"PRECISE\"` to keep the bottle in focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad843b-8855-4f30-98d5-81fb33779886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Serene outdoor garden setting, soft morning light filtered through leaves. \n",
    "        The product is placed in the middle of a white stone or marble platform, surrounded by lavender plants in soft focus. \n",
    "        Fresh dewdrops on foliage catching sunlight. Delicate morning mist in background. \n",
    "        Depth of field emphasizing product in foreground. Natural color palette with soft purples, greens, and white tones. \n",
    "        Professional product photography, 8K resolution. Golden hour lighting creating gentle rim light on bottle edge and generating a shadow of the product. \n",
    "        Hyper-realistic botanical details with macro-style flower elements. Clean, fresh atmosphere matching product aesthetic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e83372-d2f6-41f2-8e54-0d52a2cdca23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "# Set the image to be edited.\n",
    "source_image = \"example-images/baby_oil.png\"\n",
    "\n",
    "# Load and pad the image\n",
    "source_image_base64 = resize_and_pad_image(source_image, 1280, 720, 50, 50, 70)\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inference_params = {\n",
    "    \"taskType\": \"OUTPAINTING\",\n",
    "    \"outPaintingParams\": {\n",
    "        \"image\": source_image_base64,\n",
    "        \"text\": prompt,  # Description of the background to generate\n",
    "        \"maskPrompt\": \"product\",  # The element(s) to keep\n",
    "        \"outPaintingMode\": \"PRECISE\",  # \"DEFAULT\" softens the mask. \"PRECISE\" keeps it sharp.\n",
    "    },\n",
    "    \"imageGenerationConfig\": {\n",
    "        \"numberOfImages\": 1,  # Number of variations to generate. 1 to 5.\n",
    "        \"quality\": \"standard\",  # Allowed values are \"standard\" and \"premium\"\n",
    "        \"cfgScale\": 5.0,  # How closely the prompt will be followed\n",
    "        \"seed\": randint(0, 858993459),  # Use a random seed\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define an output directory with a unique name.\n",
    "generation_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_directory = f\"output/bk-replacement-{generation_id}\"\n",
    "\n",
    "# Create the generator.\n",
    "generator = BedrockImageGenerator(\n",
    "    output_directory=output_directory\n",
    ")\n",
    "\n",
    "# Generate the image(s).\n",
    "response_background_replacement = generator.generate_images(inference_params)\n",
    "\n",
    "if \"images\" in response_background_replacement:\n",
    "    # Save and display each image\n",
    "    images = file_utils.save_base64_images(response_background_replacement[\"images\"], output_directory, \"image\")\n",
    "    for image in images:\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4d073-4e56-4565-aa6c-f09bbfd5b2de",
   "metadata": {},
   "source": [
    "Now that we have the image, let's generate a video using the previous generated image as an initial starting image together with a text prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e46f8-ea14-46a8-b8df-2667259d6be0",
   "metadata": {},
   "source": [
    "## Step 2: Video Generation with Nova Reel\n",
    "\n",
    "Generating a video takes some time - approximately 3.5 minutes to produce a 6 second video. To accomodate this execution time, the Bedrock Runtime introduces a new asynchronous invocation API. Calling `start_async_invoke()` creates a new invocation job. When the job completes, Bedrock automatically saves the generated video to an S3 bucket you specify.\n",
    "\n",
    "### Image-to-Video\n",
    "\n",
    "You can also generate videos by providing an initial starting image and a text prompt. For best results, the text prompt should describe the image and also provide details about the desired action and camera movement you'd like the video to have.\n",
    "\n",
    "**NOTE:** For this use case we need a static camera shot and the product to remain still. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffbf6b-d22a-4abc-b4f7-f7ed57cbf942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VIDEO_MODEL_ID = \"amazon.nova-reel-v1:0\"\n",
    "\n",
    "# Set default region and credentials.\n",
    "boto3.setup_default_session(\n",
    "    region_name=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e564f-732f-4d56-90bf-80109b88a18b",
   "metadata": {},
   "source": [
    "Specify the name of your own Amazon S3 Bucket in the `s3_destination_bucket` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b1ed0-b032-4412-8a9a-449d1c626983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify an S3 bucket for the video output.\n",
    "s3_destination_bucket = \"bedrock-video-generation-us-east-1-hqhqna\"  # Change this to a unique bucket name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12054f-222c-4a87-974e-d19a3276a0af",
   "metadata": {},
   "source": [
    "Play around with different video prompts by modifying the `video_prompt` variable in the code below. Do NOT modify the `FIXED_PREFIX` since it is crucial to generate a static video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dadfa8-3878-4715-bb42-cd689081cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_PREFIX = \"Static camera shot. The product remains completely still.\"\n",
    "\n",
    "# Specify your video generation prompt. Phrase your prompt as a summary rather than a command. Maximum 512 characters.\n",
    "video_prompt = f\"\"\"\n",
    "{FIXED_PREFIX}\n",
    "Peaceful garden scene with gentle natural movement. Slow motion.\n",
    "Early morning atmosphere with filtered sunlight in the background.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c8fbf-b5d4-47ab-8923-a8be2f1f1b5d",
   "metadata": {},
   "source": [
    "Let's import some utils first that will help us with the video generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdc91d-0a9c-4836-8f7d-e40678758385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../../video-generation/python')\n",
    "import amazon_video_util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc04fe3-2d0f-4357-8447-c368891b8936",
   "metadata": {
    "tags": []
   },
   "source": [
    "The next code configures an asynchronous video generation process using Bedrock. It uses the background-replaced image as a reference, defines a video prompt for a static product with moving background, and initiates a 6-second video generation job with fixed dimensions and a random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd43b9d-4fd6-48c4-8cb6-c002a77e3f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import base64\n",
    "\n",
    "\n",
    "# Define the image to use as the input image.\n",
    "if \"images\" in response_background_replacement:\n",
    "    first_image_base64 = response_background_replacement[\"images\"][0]  # Gets the reference image. Must be 1280 x 720\n",
    "\n",
    "\"\"\"\n",
    "STOP: You should not have to modify anything below this line.\n",
    "\"\"\"\n",
    "\n",
    "# Set up the S3 client.\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# Create the S3 bucket.\n",
    "s3_client.create_bucket(Bucket=s3_destination_bucket)\n",
    "\n",
    "# Create the Bedrock Runtime client.\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "model_input = {\n",
    "    \"taskType\": \"TEXT_VIDEO\",\n",
    "    \"textToVideoParams\": {\n",
    "        \"text\": video_prompt,\n",
    "        \"images\": [\n",
    "            {\n",
    "                \"format\": \"png\",  # May be \"png\" or \"jpeg\"\n",
    "                \"source\": {\n",
    "                    \"bytes\": first_image_base64\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "    \"videoGenerationConfig\": {\n",
    "        \"durationSeconds\": 6,  # 6 is the only supported value currently.\n",
    "        \"fps\": 24,  # 24 is the only supported value currently.\n",
    "        \"dimension\": \"1280x720\",  # \"1280x720\" is the only supported value currently.\n",
    "        \"seed\": random.randint(\n",
    "            0, 2147483648\n",
    "        ),  # A random seed guarantees we'll get a different result each time this code runs.\n",
    "    },\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Start the asynchronous video generation job.\n",
    "    invocation = bedrock_runtime.start_async_invoke(\n",
    "        modelId=VIDEO_MODEL_ID,\n",
    "        modelInput=model_input,\n",
    "        outputDataConfig={\"s3OutputDataConfig\": {\"s3Uri\": f\"s3://{s3_destination_bucket}\"}},\n",
    "    )\n",
    "\n",
    "    # This will be used by other cells in this notebook.\n",
    "    invocation_arn = invocation[\"invocationArn\"]\n",
    "\n",
    "    # Pretty print the response JSON.\n",
    "    logger.info(\"\\nResponse:\")\n",
    "    logger.info(json.dumps(invocation, indent=2, default=str))\n",
    "\n",
    "    # Save the invocation details for later reference. Helpful for debugging and reporting feedback.\n",
    "    amazon_video_util.save_invocation_info(invocation, model_input)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbeafa1-21d8-4ec7-9b11-bbe2e6c32554",
   "metadata": {},
   "source": [
    "### Checking the status of generation jobs\n",
    "\n",
    "We've provided a set of utility functions in the `amazon_video_util.py` script. One of these functions will automatically download a job if it is completed or monitor it while it is in-progress. The `invocation_arn` is defined in the code cell above and passed in below. This function will return the local file path for the generated video for successfully completed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7997636-7274-4c37-b8d5-6930845a7d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_directory_video = amazon_video_util.monitor_and_download_video(invocation_arn, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c7e42-cca4-4bbe-9ac8-4528a04b4883",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Viewing output video\n",
    "You can then view your output video by running the below cell.\n",
    "\n",
    "> ⚠️ **Important Note:** Before proceeding with the overlay step, carefully review your generated video for any unintended camera movement. If you notice any slight camera motion or instability, regenerate the video. Any movement in the base video will contrast with the static overlay post-processing, creating an unnatural effect in the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffbe24-98ab-493d-ac48-9ae4b2792141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(output_directory_video, embed=True, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab719a3-8139-47bd-8106-594502f61b5c",
   "metadata": {},
   "source": [
    "To correct the distorted brand logos and texts, we will use a simple technique which consist in overlaying the product on top of each frame of the video. \n",
    "\n",
    "We will first remove the background from the image provided as an initial starting image for the video generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a82baf-4978-478f-ba8c-03bc5436820b",
   "metadata": {},
   "source": [
    "# Step 3: Background removal\n",
    "\n",
    "The background removal feature of the Nova Canvas model automatically isolates the subject of an image from its background, returning a PNG with 8 bit transparency.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "  <div>\n",
    "    <img src=\"example-images/background-replacement-baby_oil.png\" style=\"max-height: 300px; max-width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Input image</p>\n",
    "  </div>\n",
    "  <div>\n",
    "    <img src=\"example-images/background-removal-baby_oil.png\" style=\"max-height: 300px; max-width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Output image</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f0a3b-382d-4bf9-8af7-bb1cebfde85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import file_utils\n",
    "import logging\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "if \"images\" in response_background_replacement:\n",
    "    first_image_base64 = response_background_replacement[\"images\"][0]  # Get the first image\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inference_params = {\n",
    "    \"taskType\": \"BACKGROUND_REMOVAL\",\n",
    "    \"backgroundRemovalParams\": {\n",
    "        \"image\": first_image_base64,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define an output directory with a unique name.\n",
    "generation_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_directory_bk_removal = f\"output/bk-removal-{generation_id}\"\n",
    "\n",
    "# Create the generator.\n",
    "generator = BedrockImageGenerator(\n",
    "    output_directory=output_directory_bk_removal\n",
    ")\n",
    "\n",
    "# Generate the image(s).\n",
    "response_background_removal = generator.generate_images(inference_params)\n",
    "\n",
    "if \"images\" in response_background_removal:\n",
    "    # Save and display each image\n",
    "    images = file_utils.save_base64_images(response_background_removal[\"images\"], output_directory_bk_removal, \"image\")\n",
    "    for image in images:\n",
    "        display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5c6fe-1bb9-4184-b010-1bf16977ed43",
   "metadata": {},
   "source": [
    "# Step 4: Product Overlay and Final Video Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60861480-f726-457a-a375-f704b6d61fa6",
   "metadata": {},
   "source": [
    "This code implements a video processing workflow that handles product overlay on video frames. The script contains three main functions: `split_video_to_frames` extracts individual frames from a video file and saves them as images, `overlay_product` blends a product image with transparency onto a background frame while preserving alpha channels, and `create_video_with_overlay` combines everything by taking a folder of frames, overlaying a product image on each frame, and compiling them back into a video. \n",
    "\n",
    "The implementation uses OpenCV (cv2) for image and video processing, handling both BGR and BGRA color spaces to maintain transparency during the overlay process. [OpenCV (Open Source Computer Vision Library)](https://pypi.org/project/opencv-python/) is a powerful open-source library for computer vision and machine learning applications. The library contains over 2,500 optimized algorithms for tasks like image processing, object detection, facial recognition, and video analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4810d-fb97-43a2-b72e-65e3a9928124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split_video_to_frames(video_path, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Read the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Save frame\n",
    "        output_path = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        frame_count += 1\n",
    "        \n",
    "    cap.release()\n",
    "    return frame_count\n",
    "\n",
    "def overlay_product_simple(background, product_img):\n",
    "    # Convert background to RGBA if needed\n",
    "    if background.shape[2] == 3:\n",
    "        background = cv2.cvtColor(background, cv2.COLOR_BGR2BGRA)\n",
    "    \n",
    "    # Extract the alpha channel\n",
    "    alpha = product_img[:, :, 3] / 255.0\n",
    "    \n",
    "    # Blend the images using alpha channel\n",
    "    for c in range(3):  # BGR channels\n",
    "        background[:, :, c] = background[:, :, c] * (1 - alpha) + product_img[:, :, c] * alpha\n",
    "    \n",
    "    return background\n",
    "\n",
    "def create_video_with_overlay(frames_folder, product_path, output_path, fps=24):\n",
    "    # Read the product image (with alpha channel)\n",
    "    product = cv2.imread(product_path, cv2.IMREAD_UNCHANGED)\n",
    "    if product is None:\n",
    "        raise ValueError(\"Could not load product image\")\n",
    "    \n",
    "    # Get the first frame to determine video size\n",
    "    first_frame = cv2.imread(os.path.join(frames_folder, sorted(os.listdir(frames_folder))[0]))\n",
    "    height, width = first_frame.shape[:2]\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    for frame_file in sorted(os.listdir(frames_folder)):\n",
    "        if frame_file.endswith(('.jpg', '.png')):\n",
    "            # Read frame\n",
    "            frame_path = os.path.join(frames_folder, frame_file)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            # Overlay product\n",
    "            frame_with_overlay = overlay_product_simple(frame, product)\n",
    "            \n",
    "            # Convert back to BGR for video writing\n",
    "            if frame_with_overlay.shape[2] == 4:\n",
    "                frame_with_overlay = cv2.cvtColor(frame_with_overlay, cv2.COLOR_BGRA2BGR)\n",
    "            \n",
    "            # Write frame\n",
    "            out.write(frame_with_overlay)\n",
    "    \n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec59d7-83f6-4c5b-a602-ad38dc8e325d",
   "metadata": {},
   "source": [
    "Next, we will execute the defined functions step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8f0ac-4445-4757-91e1-b9cdb9050a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input paths\n",
    "video_path = output_directory_video\n",
    "product_path = output_directory_bk_removal + '/image_1.png'\n",
    "frames_folder = 'videos/temp_frames'\n",
    "output_path = 'videos/output_video_overlay.mp4'\n",
    "        \n",
    "# Convert base64 image to png\n",
    "response_background_removal[\"images\"][0]\n",
    "\n",
    "# 1. Split video into frames\n",
    "print(\"Splitting video into frames...\")\n",
    "frame_count = split_video_to_frames(video_path, frames_folder)\n",
    "\n",
    "# 2 & 3. Overlay product and create new video\n",
    "print(\"Creating video with overlay...\")\n",
    "create_video_with_overlay(frames_folder, product_path, output_path)\n",
    "\n",
    "# Clean up temporary frames\n",
    "print(\"Cleaning up...\")\n",
    "for frame_file in os.listdir(frames_folder):\n",
    "    os.remove(os.path.join(frames_folder, frame_file))\n",
    "os.rmdir(frames_folder)\n",
    "\n",
    "print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff171dfa-b11a-43af-991b-ff0e26b7b507",
   "metadata": {},
   "source": [
    "At last, we use the MoviePy library to convert the generated video back to H.264 format. [MoviePy](https://pypi.org/project/moviepy/) is a Python library for video editing: cuts, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912e773-396e-40ae-bbfb-310bf27512d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "def convert_to_h264(input_file, output_file):\n",
    "    clip = VideoFileClip(input_file)\n",
    "    clip.write_videofile(output_file, \n",
    "                        codec='libx264',\n",
    "                        fps=24,\n",
    "                        preset='medium',\n",
    "                        bitrate='4000k')\n",
    "    clip.close()\n",
    "\n",
    "output_path_h264 = 'videos/output_video_overlay_h264.mp4'\n",
    "convert_to_h264(output_path, output_path_h264)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196513f-29b3-428f-9748-d572a01e4bae",
   "metadata": {},
   "source": [
    "#### Great! Let's now visualize our final result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01947298-c76b-42a1-b47c-f6d45a5ec2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(output_path_h264, embed=True, height=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
