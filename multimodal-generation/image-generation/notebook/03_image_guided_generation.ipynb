{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Guided Generation\n",
    "\n",
    "This notebook demonstrates how to use the content and layout of one image to influence the generation of a new one. The input image is called a \"conditioning image\". This is sometimes referred to as \"ControlNet\". This feature can be useful to gain more direct control over things like:\n",
    "- image composition\n",
    "- camera angle\n",
    "- subject placement\n",
    "- subject pose\n",
    "\n",
    "Amazon image models support two types of conditioning image control modes:\n",
    "- \"CANNY_EDGE\" - This mode retains the prominent outlines of objects in the conditioning image. This is most useful when you want the shape of the objects in the generated image to adhere closely to those in the conditioning image.\n",
    "- \"SEGMENTATION\" - This mode allows the model more flexibility to change the shape of the objects it generates while still producing an identical image layout.\n",
    "\n",
    "The code below uses the conditioning image on the left and the prompt *\"3d animated film style, a woman with a crazy blond hair style, wearing a green sequin dress\"*. Note that with CANNY_EDGE as the control mode, it includes edge-defined details like the glasses, the coverging vertical lines in the background, and maintains the exact collar shape. With SEGMENTATION mode, the model is free to be more creative.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <div style=\"width: 31%;\">\n",
    "        <p align=\"center\">\n",
    "            <img src=\"../images/condition-image-1.png\" width=\"100%\" style=\"padding: 4px\">\n",
    "            <br>\n",
    "            <em>Condition image</em>\n",
    "        </p>\n",
    "    </div>\n",
    "    <div style=\"width: 31%;\">\n",
    "        <p align=\"center\">\n",
    "            <img src=\"../images/doc-images/image-conditioned-example-1.png\" width=\"100%\" style=\"padding: 4px\">\n",
    "            <br>\n",
    "            <em>Output - CANNY_EDGE</em>\n",
    "        </p>\n",
    "    </div>\n",
    "    <div style=\"width: 31%;\">\n",
    "        <p align=\"center\">\n",
    "            <img src=\"../images/doc-images/image-conditioned-example-2.png\" width=\"100%\" style=\"padding: 4px\">\n",
    "            <br>\n",
    "            <em>Output - SEGMENTATION</em>\n",
    "        </p>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "Experiment with changing the `controlStrength`, `controlMode`, and `text` prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from amazon_image_gen import BedrockImageGenerator\n",
    "import file_utils\n",
    "import logging\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "conditioning_image_path = \"../images/condition-image-1.png\"\n",
    "\n",
    "# Read image from file and encode it as base64 string.\n",
    "with open(conditioning_image_path, \"rb\") as image_file:\n",
    "    condition_image = base64.b64encode(image_file.read()).decode(\"utf8\")\n",
    "\n",
    "\n",
    "# Configure the inference parameters.\n",
    "inference_params = {\n",
    "    \"taskType\": \"TEXT_IMAGE\",\n",
    "    \"textToImageParams\": {\n",
    "        \"text\": \"3d animated film style, a woman with a crazy blond hair style, wearing a green sequin dress\",\n",
    "        \"conditionImage\": condition_image,\n",
    "        \"controlMode\": \"SEGMENTATION\", # \"CANNY_EDGE\" or \"SEGMENTATION\",\n",
    "        \"controlStrength\": 0.3  # How closely to match the condition image\n",
    "    },\n",
    "    \"imageGenerationConfig\": {\n",
    "        \"numberOfImages\": 1,  # Number of variations to generate. 1 to 5.\n",
    "        \"quality\": \"standard\",  # Allowed values are \"standard\" and \"premium\"\n",
    "        \"width\": 1280,  # See README for supported output resolutions\n",
    "        \"height\": 720,  # See README for supported output resolutions\n",
    "        \"cfgScale\": 8.0,  # How closely the prompt will be followed\n",
    "        \"seed\": randint(0, 858993459),  # Use a random seed\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define an output directory with a unique name.\n",
    "generation_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_directory = f\"output/{generation_id}\"\n",
    "\n",
    "# Create the generator.\n",
    "generator = BedrockImageGenerator(\n",
    "    output_directory=output_directory\n",
    ")\n",
    "\n",
    "# Generate the image(s).\n",
    "response = generator.generate_images(inference_params)\n",
    "\n",
    "if \"images\" in response:\n",
    "    # Save and display each image\n",
    "    images = file_utils.save_base64_images(response[\"images\"], output_directory, \"image\")\n",
    "    for image in images:\n",
    "        display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
