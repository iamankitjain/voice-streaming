{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c16187d38c2af6",
   "metadata": {},
   "source": [
    "# Multimodal Understanding with Amazon Nova Models\n",
    "This notebook showcases how the Amazon models are able to understand and answer questions about videos and images.\n",
    "\n",
    "You can upload a video or image and ask the model questions. We can try this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108788bbd553000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f078df5f6e487a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:15:20.948512Z",
     "start_time": "2024-11-23T03:15:20.940772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"the-sea.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"the-sea.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9ac97038c6cde",
   "metadata": {},
   "source": [
    "We can read the file into bytes which is used to pass into the `Converse` API as bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ea5e44fb447a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:07:37.615293Z",
     "start_time": "2024-11-23T03:07:37.610625Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./the-sea.mp4\", \"rb\") as file:\n",
    "    media_bytes = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae2afc215f770a",
   "metadata": {},
   "source": [
    "With the data in bytes, we can pass it straight into the Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db52c0feb63419",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:08:35.262756Z",
     "start_time": "2024-11-23T03:08:30.766621Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"video\": {\"format\": \"mp4\", \"source\": {\"bytes\": media_bytes}}},\n",
    "            {\"text\": \"What is happening in this video?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.converse(modelId=\"us.amazon.nova-lite-v1:0\", messages=messages)\n",
    "print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bf59bfb06c9bb",
   "metadata": {},
   "source": [
    "The video may not always be an appropriate resolution to use as an input for the models. As such, we can easily downscale using ffmpeg-python which will upload the file to S3 where the model can read it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fea7f490369b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:08:45.401410Z",
     "start_time": "2024-11-23T03:08:42.197934Z"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from utils import resize_video\n",
    "\n",
    "bucket_name = sagemaker.session.Session().default_bucket()\n",
    "\n",
    "input_s3_uri = resize_video(media_bytes, bucket_name)\n",
    "input_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5bad95501fc49",
   "metadata": {},
   "source": [
    "With an S3 URI, we can pass this into the API and see we get a similar output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99fa9f5217ddd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:12:09.958833Z",
     "start_time": "2024-11-23T03:12:09.381358Z"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\"s3Location\": {\"uri\": input_s3_uri}},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"What is happening in this video?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.converse(modelId=\"us.amazon.nova-lite-v1:0\", messages=messages)\n",
    "response[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9564e20817967b8",
   "metadata": {},
   "source": [
    "These models will sample a particular number of frames to be used as input alongside our prompt.\n",
    "\n",
    "Using a helper library we can complete a similar task, however only up to 20 images are supported by the Converse API so results may vary.\n",
    "\n",
    "Using a helper library, this may look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e054a8dc2e09520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:22:35.617565Z",
     "start_time": "2024-11-23T03:22:34.932257Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils import resample_video_to_frames, convert_frames_to_converse_format\n",
    "\n",
    "frames = resample_video_to_frames(media_bytes)\n",
    "print(f\"The video now has {len(frames)} frames\")\n",
    "plt.imshow(frames[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeca20c5424a76",
   "metadata": {},
   "source": [
    "In order to be valid for the Converse API, we can convert each frame to a bytes representation to be passed into the Converse API to invoke the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62f812c2b2ace5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:24:56.931802Z",
     "start_time": "2024-11-23T03:24:52.570341Z"
    }
   },
   "outputs": [],
   "source": [
    "converted_frames = convert_frames_to_converse_format(frames)\n",
    "\n",
    "content = [\n",
    "    {\"image\": {\"format\": \"jpeg\", \"source\": {\"bytes\": frame_bytes}}}\n",
    "    for frame_bytes in converted_frames\n",
    "]\n",
    "content.append({\"text\": \"What is happening in the video?\"})\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\"s3Location\": {\"uri\": input_s3_uri}},\n",
    "                }\n",
    "            },\n",
    "            {\"text\": \"What is happening in this video?\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.converse(modelId=\"us.amazon.nova-lite-v1:0\", messages=messages)\n",
    "response[\"output\"][\"message\"][\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f20537c4db1ec2",
   "metadata": {},
   "source": [
    "Since the model will sample frames from our video, we may want to understand what this will look like for a video of an arbitrary length, or we might want to get an estimation of how many tokens our video might use.\n",
    "\n",
    "We can plot these values using a function like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b773a570102839c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:28:11.250244Z",
     "start_time": "2024-11-23T03:28:10.630060Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import plot_samples_fps_tokens\n",
    "\n",
    "plot_samples_fps_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0aabc5e15aad03",
   "metadata": {},
   "source": [
    "We can see that after a certain point, the model samples at most 960 frames which is equivalent to 276,480 input tokens and the FPS sampled will reduce to keep this consistent.\n",
    "\n",
    "With these numbers, we can roughly estimate our frame sample rate, FPS and token count for our video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e05eccfd0f8f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T03:31:26.292075Z",
     "start_time": "2024-11-23T03:31:26.288612Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import get_sampled_fps, get_sampled_frame_count, get_sampled_tokens\n",
    "\n",
    "duration_in_seconds = 240\n",
    "\n",
    "print(\n",
    "    f\"Our video which is {duration_in_seconds} long will be sampled at a rate of {get_sampled_fps(duration_in_seconds)} FPS for a frame count of {get_sampled_frame_count(duration_in_seconds)} which will use around {get_sampled_tokens(duration_in_seconds)} tokens\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
