{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e0f5129-f521-4118-b17b-4fc0d5e079c3",
   "metadata": {},
   "source": [
    "# Key Information Extraction to SQL Analysis\n",
    "(Doc Understanding - FinQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adea76e-e44d-409a-bd5e-0e3d71c6ab7d",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Introduction and Prerequisites](#introduction)\n",
    "2. [Data and Model Preparation](#paragraph1)\n",
    "    - 2.1 Setup boto3 And Model\n",
    "    - 2.2 Download Data From Huggingface\n",
    "    - 2.3 Result Post-processing Preparation\n",
    "    - 2.4 Ground Turth Data Preparation\n",
    "3. [Model Execution and Evaluation](#paragraph2)\n",
    "    - 3.1 Model Execute Preparation\n",
    "    - 3.2 Model Evaluation Preparation\n",
    "    - 3.3 Model Execution\n",
    "4. [Output Analysis Using SQL](#paragraph3)\n",
    "    - 4.1 Create Table in GLue Data Catalog\n",
    "    - 4.2 Query on Athena using SQL\n",
    "5. [(Optional) Model Comparison](#paragraph4)\n",
    "    - 5.1 Sonnet 3\n",
    "    - 5.2 Nova lite\n",
    "    - 5.3 Haiku 3\n",
    "    - 5.4 Accuracy and Exectuion Time Comparison\n",
    "    - 5.5 Latency and Token Usage Comparison\n",
    "6. [Clean Up](#paragraph5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349511f-3811-4716-a225-90bc77552b6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Introduction and Prerequisites <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fca66a-09b2-4020-ad29-f43fedde3d73",
   "metadata": {},
   "source": [
    "This notebook demonstrates a proof of concept for processing invoice images utilising GenAI, with the aim of generating structured output that can be seamlessly consumed by downstream SQL engines.\n",
    "\n",
    "![Architecture](images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed1f84-7ff2-4685-9b4d-591bb27e10f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Prerequisites**\n",
    "- Bedrock Nova Pro [model access](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html)\n",
    "- (Optional) Bedrock Claude 3 Sonnet,Claude 3 Haiku and Nova Lite model access.\n",
    "- A AWS [Glue service role](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html).\n",
    "- The Sagemaker exectuion role used in this notebook needs to have [AmazonSageMakerFullAccess](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonSageMakerFullAccess.html) policy and [turst relationship](https://docs.aws.amazon.com/directoryservice/latest/admin-guide/edit_trust.html) with sagemaker service. Replace `\"Service\": \"ds.amazonaws.com\"` to `\"Service\": \"sagemaker.amazonaws.com\"` .\n",
    "- The role above has to have related S3 permissions to create and access the s3 bucket. For example, you can give [AmazonS3FullAccess](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-amazons3fullaccess).\n",
    "- The role above has to have Bedrock model access permissions. For example, you can give [AmazonBedrockFullAccess](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonBedrockFullAccess.html).\n",
    "- - The role above needs to have permissions to create database, table, cralwer and run crawler permission. For example, you can give [AWSGlueConsoleFullAccess](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AWSGlueConsoleFullAccess.html)\n",
    "- The role above needs to have an [iam:PassRole](https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html) permission to use the Glue service role.\n",
    "~~~\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"ARN OF YOUR GLUE SERVICE ROLE\"\n",
    "        }\n",
    "    ]\n",
    "}                                           \n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a658a6-4720-492f-9829-f8fa15e01c23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Data and Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba9bc2-2975-4031-8aaf-bce7bfa706b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Setup boto3 And Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950126e-2a8f-499b-913f-5dabd7f89685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# create model id vars\n",
    "micro = 'us.amazon.nova-micro-v1:0'\n",
    "lite = 'us.amazon.nova-lite-v1:0'\n",
    "pro= 'us.amazon.nova-pro-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f10e4f-97de-4af5-a130-9cb694221d2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2 Download Data From Huggingface\n",
    "\n",
    "[This dataset](https://huggingface.co/datasets/katanaml-org/invoices-donut-data-v1) contains invoice documents annotated and processed to be ready for Donut ML model fine-tuning. Annotation and data preparation task was done by Katana ML team. For this workshop, we are using a small set of the data for demonstration purposes.\n",
    "\n",
    "In the dataset, it has two columns. The **image** column already has the image file ocnverted to bytes. The **ground_truth** column holds the actaul contect of the recipt, which will be used to evaluate model results in the later section.\n",
    "\n",
    "Original dataset info: Kozłowski, Marek; Weichbroth, Paweł (2021), “Samples of electronic invoices”, Mendeley Data, V2, doi: 10.17632/tnj49gpmtz.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee263fb1-bc04-4a25-9010-3338aa158114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TO DO: uncomment this if never run this before\n",
    "#! pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9097948c-f685-44e3-8c2f-89c695ad3f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download data from huggingface \n",
    "splits = {'train': 'data/train-00000-of-00001-a5c51039eab2980a.parquet', 'validation': 'data/validation-00000-of-00001-b8a5c4a6237baf25.parquet', 'test': 'data/test-00000-of-00001-56af6bd5ff7eb34d.parquet'}\n",
    "# extract test dataset out\n",
    "dftest = pd.read_parquet(\"hf://datasets/katanaml-org/invoices-donut-data-v1/\" + splits[\"test\"])\n",
    "#save downloaded files to local\n",
    "dftest.to_parquet('testdataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d48c2-3d76-4bc7-915e-92573ecec526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Have a grasp of the data\n",
    "print(f\"The number of sample invoices: {len(dftest)}\")\n",
    "print(\"-------------- Data Preview --------------\")\n",
    "dftest.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7a624-a3f9-4642-b1c5-4c3ceddff483",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.3 Result Post-processing Preparation\n",
    "\n",
    "In the next section, it will ask the foundation model to return the results in a json format. However, in case extra charachters are returned, the `extract_json_objects` function will only extract the JSON part. Then `json_list_to_dataframe` function will convert the JSON objects into a dataframe, which is going to be fed into downstream SQL engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b61b2f-cbe6-40a2-893a-060c81c0ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_json_objects(input_string):\n",
    "    # Remove the code block markers and any leading/trailing whitespace\n",
    "    json_string = re.sub(r'```json\\n|\\n```', '', input_string).strip()\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON string into a Python object\n",
    "        json_data = json.loads(json_string)\n",
    "        \n",
    "        # Check if the parsed data is a list\n",
    "        if isinstance(json_data, list):\n",
    "            return json_data\n",
    "        else:\n",
    "            raise ValueError(\"The extracted JSON is not a list of objects\")\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4028c3-f075-44d1-b569-f1527e4803e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the json list into a dataframe\n",
    "def json_list_to_dataframe(json_list):\n",
    "    \"\"\"\n",
    "    Convert a list of JSON objects to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        json_list (list): A list of JSON objects (dictionaries).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame constructed from the list of JSON objects.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # If the input list is empty, return the empty DataFrame\n",
    "    if not json_list:\n",
    "        return df\n",
    "\n",
    "    # Extract the keys from the first JSON object\n",
    "    keys = list(json_list[0].keys())\n",
    "\n",
    "    # Create a DataFrame from the list of JSON objects\n",
    "    df = pd.DataFrame(json_list, columns=keys)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5cc71-b85c-47d6-98e3-6fc6fe1d69d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.4 Ground Turth Data Preparation\n",
    "\n",
    "The `ground_truth` function below extracts the ground truth column, which will later be used to compare with the model output to evaluate the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475acd4-92cf-4633-b79a-d2f4f4e694ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the Ground Truth data into a dataframe\n",
    "def ground_truth(gt_data):\n",
    "    # Parse the JSON string\n",
    "    data = json.loads(gt_data)\n",
    "\n",
    "    # Extract the necessary information\n",
    "    header = data.get('gt_parse', {}).get('header', {})\n",
    "    items = data.get('gt_parse', {}).get('items', [])\n",
    "\n",
    "    # Create a list of dictionaries for the DataFrame\n",
    "    records = []\n",
    "    for item in items:\n",
    "        record = {\n",
    "            'invoice_number': header.get('invoice_no', ''),\n",
    "            'date_of_issue': header.get('invoice_date', ''),\n",
    "            'seller_name': header.get('seller', ''),\n",
    "            'client_name': header.get('client', ''),\n",
    "            'item_description': item.get('item_desc', ''),\n",
    "            'quantity': get_float_value(item.get('item_qty', '0'), remove_spaces=True),\n",
    "            'net_price': get_float_value(item.get('item_net_price', '0'), remove_spaces=True),\n",
    "            'vat_percent': item.get('item_vat', item.get('iban',0)),\n",
    "            'gross_worth': get_float_value(item.get('item_gross_worth', '0'), remove_spaces=True),\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    gt_df = pd.DataFrame(records)\n",
    "    return gt_df\n",
    "\n",
    "def get_float_value(value, remove_spaces=False):\n",
    "    if remove_spaces:\n",
    "        value = value.replace(' ', '')\n",
    "    try:\n",
    "        return float(value.replace(',', '.'))\n",
    "    except ValueError:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c698020-d6f6-46cd-b868-12a9bc631845",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Model Execution and Evaluation <a name=\"paragraph1\"></a>\n",
    "This section define the functions that run invoice images OCR use `model_id` specified. The model results is collected which is later used to comparewith the ground truth data, and evaluate the model's performance by calcuating the accuracy rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dca87a-1546-4f4a-8658-17b73faa6566",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Model Execute Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4700ba5-8396-4f56-ac8d-a331b2114a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def run_model(model_id, system_list,prompt_text, image_df, inf_params, client):\n",
    "    \n",
    "    final_result_df_list = []\n",
    "    final_gt_df_list = []\n",
    "    final_mdoel_eval_df_list = []\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "\n",
    "    for index, row in image_df.iterrows():\n",
    "        \n",
    "        imagedata = row[\"image\"]['bytes']\n",
    "        print(f\"Processing invoice {index}\")\n",
    "        \n",
    "        #execute model\n",
    "        message_list = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": \"jpeg\",\n",
    "                        \"source\": {\"bytes\": imagedata},\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": prompt_text\n",
    "                }\n",
    "            ],\n",
    "        }]\n",
    "\n",
    "\n",
    "        response = client.converse(modelId=model_id, \n",
    "                                   messages=message_list, \n",
    "                                   system = system_list, \n",
    "                                   inferenceConfig = inf_params)\n",
    "\n",
    "        content_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        # print(content_text)\n",
    "        \n",
    "         # post processing the result\n",
    "        json_lst = extract_json_objects(content_text)\n",
    "        result_df = json_list_to_dataframe(json_lst)\n",
    "        \n",
    "        #add metrics\n",
    "        result_df['input_token_usage'] = int(response['usage']['inputTokens'])\n",
    "        result_df['output_token_usage'] = int(response['usage']['outputTokens'])\n",
    "        result_df['model_latency'] = int(response['metrics']['latencyMs'])\n",
    "        \n",
    "        final_result_df_list.append(result_df)\n",
    "        \n",
    "        # get the ground truth data\n",
    "        gtdata = row[\"ground_truth\"]\n",
    "        # print(gtdata)\n",
    "        gt_df = ground_truth(gtdata)\n",
    "        final_gt_df_list.append(gt_df)\n",
    "        \n",
    "        # compare the model results with gt results\n",
    "        mdoel_eval_df = model_evaluation(result_df, gt_df)\n",
    "        final_mdoel_eval_df_list.append(mdoel_eval_df)\n",
    "    \n",
    "    end = time.time()\n",
    "    execution_time = end - start\n",
    "    print(\"Model: \",model_id, \"took\", execution_time, \"seconds to finish processing\", len(image_df), \"invoice pictures\")\n",
    "    \n",
    "    final_result_df = pd.concat(final_result_df_list, axis=0, ignore_index=True)   \n",
    "    final_gt_df = pd.concat(final_gt_df_list, axis=0, ignore_index=True)\n",
    "    final_mdoel_eval_df = pd.concat(final_mdoel_eval_df_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    return final_result_df,final_gt_df,final_mdoel_eval_df,execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9aef09-bce5-4812-8e59-f73cd6ab4cec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Model Evaluation Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6a183-7a0e-4db4-a241-6c62012b7fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine the model results with GroundTruth for comparation\n",
    "def model_evaluation(result_df, gt_df):\n",
    "    # Reset the index of both dataframes\n",
    "    result_df = result_df.reset_index()\n",
    "    gt_df = gt_df.reset_index()\n",
    "    \n",
    "    # Merge the dataframes on 'Invoice Number' and index\n",
    "    try:\n",
    "        merged_df = result_df.merge(gt_df, on=['index'], how='left', suffixes=('_result', '_gt'))\n",
    "    except:\n",
    "        result_df['invoice_number'] = result_df['invoice_number'].astype(str)\n",
    "        result_df['index'] = result_df['index'].astype(str)\n",
    "        gt_df['invoice_number'] = gt_df['invoice_number'].astype(str)\n",
    "        gt_df['index'] = gt_df['index'].astype(str)\n",
    "        merged_df = result_df.merge(gt_df, on=[ 'index'], how='left', suffixes=('_result', '_gt'))\n",
    "    # Rename the columns to distinguish them\n",
    "    renamed_cols = {f'{col}_result': f'{col}_result' if col != 'index' else col for col in merged_df.columns}\n",
    "    renamed_cols.update({f'{col}_gt': f'{col}_gt' if col != 'index' else col for col in merged_df.columns})\n",
    "    merged_df = merged_df.rename(columns=renamed_cols)\n",
    "    \n",
    "    # Rearrange the columns to make them more comparable\n",
    "    cols_to_keep = [\n",
    "        'invoice_number_result', \n",
    "        'invoice_number_gt',\n",
    "        'date_of_issue_result', \n",
    "        'date_of_issue_gt', \n",
    "        'seller_name_result', \n",
    "        'seller_name_gt', \n",
    "        'client_name_result', \n",
    "        'client_name_gt', \n",
    "        'item_description_result', \n",
    "        'item_description_gt', \n",
    "        'quantity_result', \n",
    "        'quantity_gt', \n",
    "        'net_price_result', \n",
    "        'net_price_gt', \n",
    "        'vat_percent_result', \n",
    "        'vat_percent_gt', \n",
    "        'gross_worth_result', \n",
    "        'gross_worth_gt']\n",
    "    merged_df = merged_df[cols_to_keep]\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8f200-451a-46a4-af06-1cfbc9a9cf46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_score(final_mdoel_eval_df):\n",
    "    # add column names that you want to evaluate\n",
    "    eval_list = [\"invoice_number\", \"quantity\", \"net_price\", \"vat_percent\", \"gross_worth\"]\n",
    "    # compare if model result is consistent with ground truth\n",
    "    for col in eval_list:\n",
    "        if col in ['vat_percent', 'invoice_number'] :\n",
    "            final_mdoel_eval_df[col+ \"_diff\"] = final_mdoel_eval_df[col + \"_result\"] == final_mdoel_eval_df[col + \"_gt\"]\n",
    "        else: \n",
    "            difftest = final_mdoel_eval_df[col + \"_result\"].astype(float) - final_mdoel_eval_df[col + \"_gt\"].astype(float)\n",
    "            final_mdoel_eval_df[col + \"_diff\"] = (difftest == 0)\n",
    "            \n",
    "    # return the rows that the model results and ground truth are not consistent\n",
    "    df_wrong = final_mdoel_eval_df[final_mdoel_eval_df.eq(False).any(axis=1)]\n",
    "    accuracy = (1 - len(df_wrong)/len(final_mdoel_eval_df)) * 100\n",
    "    print(f\"Accuracy is {accuracy}%\")\n",
    "    return accuracy, df_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe6bc9-9bc0-488e-b526-c068154eb46a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Model Execution\n",
    "This section will utilise all the functions we define above to invoke model. The accuracy is calculated and the OCR result is stored as `output.parquet` file in s3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274dc7a-d9b3-4df4-a14a-5a71eea26afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "config = Config(retries = {'max_attempts': 10,'mode': 'adaptive'})\n",
    "client = boto3.client(\"bedrock-runtime\",region_name=\"us-east-1\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b5ee0-8874-441f-ae54-0c294906df61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_df = dftest.copy()\n",
    "# to run on a small scale\n",
    "# image_df = dftest.iloc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4be896-f365-4de8-a930-c4ba37bd4fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the parameters\n",
    "system_list = [\n",
    "    {\n",
    "        \"text\": \"You are an data expert who is good at analysing the data and design relational databases\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_text = '''\n",
    "please go through the full picture and provide ALL the contect and items you see. Missing item will be penalised.\n",
    "\n",
    "The image is a invoice document and please extract below informiaton in the form of JSON. \n",
    "Each item should return one JSON object, and if there are multiple items in the invoice, retrun a list of JSON objects.\n",
    "\n",
    "The result has to follow below example structure:\n",
    "**Example 1:**\n",
    "[{\"invoice_number\": \"65321852\",\n",
    "\t\"date_of_issue\": \"04/11/2021\",\n",
    "\t\"seller_name\": \"Kaufman Cooper and Young\",\n",
    "\t\"client_name\": \"Wells Carlson\",\n",
    "\t\"item_description\": \"New KID CUDI Size S to 3XL\",\n",
    "\t\"quantity\": 2.00,\n",
    "\t\"net_price\": 22.49,\n",
    "    \"vat_percent\": \"10%\",\n",
    "    \"gross_worth\": 49.48\n",
    "    },\n",
    "    {\"invoice_number\": \"65321852\",\n",
    "\t\"date_of_issue\": \"04/11/2021\",\n",
    "\t\"seller_name\": \"Kaufman Cooper and Young\",\n",
    "\t\"client_name\": \"Wells Carlson\",\n",
    "\t\"item_description\": \"New KID CUDI Size S to 3XL\",\n",
    "\t\"quantity\": 2.00,\n",
    "\t\"net_price\": 22.49,\n",
    "    \"vat_percent\": \"10%\",\n",
    "    \"gross_worth\": 49.48\n",
    "    }\n",
    "    ]\n",
    "\n",
    "Now, strictly using the examples format above, and retrun a list of JSON objects. Please keep value decimal as it, do not round up or down anything.\n",
    "All property names must be enclosed in double quotes.\n",
    "If threre's any special charachters in the item_description, such as \"/\", \"\\\", and \",\", please remove them.\n",
    "The JSON string should be valid and properly formatted, ready to be parsed by JSON.parse() in JavaScript or json.loads() in Python.\n",
    "Check the JSON string are proper JSON format; if not, please fix it.\n",
    "The \"vat_percent\" key stands for value-added tax, which is always in percentage (0% - 100%). If the recipt only shows a number, add \"%\" to the value.\n",
    "For \"seller_name\" and \"client_name\" do not include address, just output the company name.\n",
    "Do not give filed that not listed above. Do not output any explaination or other text other than the json.\n",
    "\n",
    "'''\n",
    "\n",
    "inf_params = {\"maxTokens\": 3000, \"topP\": 0.1, \"temperature\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7936b4-39e9-4870-b052-af479b48a5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_id = pro\n",
    "\n",
    "'''\n",
    "Execute model to extract info and construct formated csv output\n",
    "The output contains 3 tables, \n",
    "\n",
    "1. model final_result_df, \n",
    "2. groundtruth final_gt_df, \n",
    "3. concate table of model final_result_df and groundtruth\n",
    "\n",
    "and timespent\n",
    "'''\n",
    "\n",
    "nova_final_result_df,\\\n",
    "nova_final_gt_df,\\\n",
    "nova_final_mdoel_eval_df,\\\n",
    "nova_exe_time = run_model(model_id, \n",
    "                         system_list,prompt_text, \n",
    "                         image_df, \n",
    "                         inf_params, \n",
    "                         client)\n",
    "\n",
    "accuracy, df_wrong = model_score(nova_final_mdoel_eval_df)\n",
    "\n",
    "Nova = {\n",
    "\"model_id\":model_id,\n",
    "\"accuracy(%)\":accuracy,\n",
    "\"execution_time(s)\": nova_exe_time\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca104f7e-7179-403a-a421-fa05b8ef70ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Create Table in Glue Data Catalog\n",
    "\n",
    "In this section, the model result from the previous section will be saved into a S3 location. For this, a S3 bucket will be created. Then a Glue database and a crawler will be created. The Glue cralwer will crawl the data from the S3 location, and create a table on Glue and Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa0129-0546-43d7-a612-dbb37607ee93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "myuuid = uuid.uuid4()\n",
    "\n",
    "# create S3 bucket\n",
    "s3_client = boto3.client(\"s3\")\n",
    "bucket_name = f\"bedrock-invoice-{myuuid}\"\n",
    "prefix_name = \"invoice\"\n",
    "s3_client.create_bucket(Bucket=bucket_name)\n",
    "print(f\"AWS S3 bucket '{bucket_name}' created succesfully.\")\n",
    "\n",
    "# Create the database\n",
    "glue_client = boto3.client(\"glue\")\n",
    "database_name = \"bedrock_invoice_db\"\n",
    "glue_client.create_database(DatabaseInput={'Name': database_name})\n",
    "print(f\"AWS Glue database '{database_name}' created succesfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f3e70-d452-425f-bd83-9a77a71640eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the result to the S3 bucket\n",
    "s3_target_path = f\"s3://{bucket_name}/{prefix_name}/\"\n",
    "final_output = nova_final_result_df.drop(['input_token_usage', 'output_token_usage','model_latency'], axis = 1)\n",
    "final_output.to_parquet(f\"{s3_target_path}output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dcd44f-ded1-410f-9f5a-9c5a40d570ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the crawler\n",
    "# Add the Glue service role here created in prerequisites \n",
    "glue_svc_role = \"YOUR_GLUE_SERVICE_ROLE_ARN\"\n",
    "crawler_name = 'bedrock-invoice-crawler'\n",
    "response = glue_client.create_crawler(\n",
    "    Name=crawler_name,\n",
    "    Role=glue_svc_role,\n",
    "    DatabaseName=database_name,\n",
    "    Description='Crawler for S3 data',\n",
    "    Targets={\n",
    "        'S3Targets': [\n",
    "            {'Path': s3_target_path}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "time.sleep(5)\n",
    "print(f\"AWS Glue Crawler '{crawler_name}' created succesfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b397d3e-6a66-4c2e-b4ef-5a2ebde13ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Glue Crawler. This step is going to take a few minutes to complete. \n",
    "response = glue_client.start_crawler(\n",
    "    Name=crawler_name\n",
    ")\n",
    "print(f\"AWS Glue Crawler '{crawler_name}' started.\")\n",
    "time.sleep(5)\n",
    "state_previous = None\n",
    "while True:\n",
    "    response_get = glue_client.get_crawler(Name=crawler_name)\n",
    "    state = response_get[\"Crawler\"][\"State\"]\n",
    "    if state != state_previous:\n",
    "        state_previous = state\n",
    "    if state == \"READY\":  # Other known states: RUNNING, STOPPING\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"AWS Glue Crawler '{crawler_name}' finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a0e7b-17a7-49eb-9cdc-b26c7f677b07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Query on Athena using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381dd04-8a11-46a4-a35d-770ac2622e74",
   "metadata": {},
   "source": [
    "Go to Athena console, and check the table has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05298a3-2987-4299-bd69-1a8380ac0cb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Athena](images/athena_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273650c-6d3f-4af9-88ae-25ba545a54f4",
   "metadata": {},
   "source": [
    "**Analysis one: find the total sales for seller Jackson Ltd**\n",
    "\n",
    "~~~\n",
    "SELECT ROUND(SUM(gross_worth), 2) AS total_sales FROM invoice \n",
    "WHERE seller_name = 'Jackson Ltd'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c22a93-e13e-4d50-9dfd-cdfa99e959a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Athena](images/athena_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae65e559-6915-48bf-b0a8-527a264ef9d8",
   "metadata": {},
   "source": [
    "**Analysis two: find all the items sold by 'Jackson Ltd'**\n",
    "\n",
    "~~~\n",
    "SELECT DISTINCT item_description FROM invoice \n",
    "WHERE seller_name = 'Jackson Ltd'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88453f35-c0eb-4623-ab24-e9c1871e5181",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Athena](images/athena_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c21b4-3e93-4802-8fe4-ae8d0f45d27e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5.(Optional) Model Comparison \n",
    "Below section will compare the performance of the Claude Sonnet 3 model, Nova Pro model, Nova Lite model and Claude Haiku 3 model on processing results using the same dataset. The metrics we will focus on are `accuracy` and model `exectuion time`, `output_token_usage` (from a cost perspective) and `model_latency` (from a performance perspective) on each invoice level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7242d-91be-4b11-b248-4a4aac5de869",
   "metadata": {},
   "source": [
    "### 5.1 Sonnet 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0b513e-498e-4aff-9478-22e3c77ad8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_id = 'us.anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "# run model\n",
    "sonnet3_final_result_df,\\\n",
    "sonnet3_final_gt_df,\\\n",
    "sonnet3_final_mdoel_eval_df,\\\n",
    "sonnet3_exe_time = run_model(model_id, \n",
    "                           system_list,\n",
    "                           prompt_text, \n",
    "                           image_df, \n",
    "                           inf_params, \n",
    "                           client)\n",
    "\n",
    "accuracy, df_wrong = model_score(sonnet3_final_mdoel_eval_df)\n",
    "sonnet3 = {\n",
    "\"model_id\":model_id,\n",
    "\"accuracy(%)\":accuracy,\n",
    "\"execution_time(s)\": sonnet3_exe_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d904a-4cd9-4f3c-ae44-ddace4c88e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print out the records that model results are inconsistent with ground truth\n",
    "# df_wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ff104-2527-493b-9e5a-6cc5c1c4bf65",
   "metadata": {},
   "source": [
    "### 5.2 Nova lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d7942-5521-4cf4-bfe2-9b2067f16199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_id = lite\n",
    "\n",
    "# run model\n",
    "novalite_final_result_df,\\\n",
    "novalite_final_gt_df, \\\n",
    "novalite_final_mdoel_eval_df,\\\n",
    "novalite_exe_time = run_model(model_id, \n",
    "                               system_list,\n",
    "                               prompt_text, \n",
    "                               image_df, \n",
    "                               inf_params, \n",
    "                               client)\n",
    "\n",
    "accuracy, df_wrong = model_score(novalite_final_mdoel_eval_df)\n",
    "novalite = {\n",
    "\"model_id\":model_id,\n",
    "\"accuracy(%)\":accuracy,\n",
    "\"execution_time(s)\": novalite_exe_time\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca498d-b7f0-4862-b336-185e675e1f73",
   "metadata": {},
   "source": [
    "### 5.3 Haiku 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e9dcb-7396-4a71-8d21-3aa17ca36c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose model\n",
    "model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "# run model\n",
    "haiku3_final_result_df,\\\n",
    "haiku3_final_gt_df, \\\n",
    "haiku3_final_mdoel_eval_df,\\\n",
    "haiku3_exe_time = run_model(model_id, \n",
    "                               system_list,\n",
    "                               prompt_text, \n",
    "                               image_df, \n",
    "                               inf_params, \n",
    "                               client)\n",
    "\n",
    "accuracy, df_wrong = model_score(haiku3_final_mdoel_eval_df)\n",
    "haiku3 = {\n",
    "\"model_id\":model_id,\n",
    "\"accuracy(%)\":accuracy,\n",
    "\"execution_time(s)\": haiku3_exe_time\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec946c5f-39d8-43ae-b3a6-21a3b29dac8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4 Accuracy and Exectuion Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b580bc-4e15-4b0f-b377-260f69b564b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add or remove model metrics from the list below to show the final metrics comparison\n",
    "comparison_list = [sonnet3, Nova, novalite, haiku3]\n",
    "model_comp = json_list_to_dataframe(comparison_list)\n",
    "model_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56fccca-f5c3-4b61-ad45-27e0db7d81d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.5 Latency and Token Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2ce5b-0427-41d0-b7fd-8893cfb000bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({'invoice_number': nova_final_result_df['invoice_number'], \n",
    "                           \n",
    "                           'sonnet3_output_token_usage': sonnet3_final_result_df['output_token_usage'], \n",
    "                           'novapro_output_token_usage': nova_final_result_df['output_token_usage'], \n",
    "                           'novalite_output_token_usage': novalite_final_result_df['output_token_usage'], \n",
    "                           'haiku3_output_token_usage': haiku3_final_result_df['output_token_usage'], \n",
    "                           \n",
    "                           'sonnet3_input_token_usage': sonnet3_final_result_df['input_token_usage'], \n",
    "                           'novapro_input_token_usage': nova_final_result_df['input_token_usage'], \n",
    "                           'novalite_input_token_usage': novalite_final_result_df['input_token_usage'],\n",
    "                           'haiku3_input_token_usage': haiku3_final_result_df['input_token_usage'],\n",
    "                           \n",
    "                           'sonnet3_model_latency': sonnet3_final_result_df['model_latency'],\n",
    "                           'novapro_model_latency': nova_final_result_df['model_latency'],\n",
    "                           'novalite_model_latency': novalite_final_result_df['model_latency'],\n",
    "                           'haiku3_model_latency': haiku3_final_result_df['model_latency'],\n",
    "                           \n",
    "                          })\n",
    "\n",
    "#group table value by invoice number \n",
    "grouped = metrics_df.groupby('invoice_number').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c2b9c-bd92-4c54-895b-fb834ce0b06b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change 'invoice_number' to string\n",
    "metrics_df['invoice_number'] = metrics_df['invoice_number'].astype(str)\n",
    "\n",
    "# create two fig\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# plot Token Usage comparison fig\n",
    "ax1.plot(metrics_df['invoice_number'], metrics_df['sonnet3_output_token_usage'], marker='o', color='red', label='sonnet3')\n",
    "ax1.plot(metrics_df['invoice_number'], metrics_df['novapro_output_token_usage'], marker='o', color='yellow', label='Novapro')\n",
    "ax1.plot(metrics_df['invoice_number'], metrics_df['novalite_output_token_usage'], marker='o', color='green', label='Novalite')\n",
    "ax1.plot(metrics_df['invoice_number'], metrics_df['haiku3_output_token_usage'], marker='o', color='blue', label='haiku3')\n",
    "\n",
    "ax1.axhline(y=metrics_df['sonnet3_output_token_usage'].mean(), color='lightcoral', linestyle='--', label='sonnet3 Mean') \n",
    "ax1.axhline(y=metrics_df['novapro_output_token_usage'].mean(), color='yellow', linestyle='--', label='Nova pro Mean') \n",
    "ax1.axhline(y=metrics_df['novalite_output_token_usage'].mean(), color='lightgreen', linestyle='--', label='Nova lite Mean') \n",
    "ax1.axhline(y=metrics_df['haiku3_output_token_usage'].mean(), color='blue', linestyle='--', label='haiku3 Mean') \n",
    "\n",
    "\n",
    "ax1.set_xlabel('Invoice Number')\n",
    "ax1.set_ylabel('Output Token Usage')\n",
    "ax1.set_title('Output Token Usage Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "ax1.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# plot Model Latency comparison fig\n",
    "ax2.plot(metrics_df['invoice_number'], metrics_df['sonnet3_model_latency'], marker='o', color='red', label='sonnet3')\n",
    "ax2.plot(metrics_df['invoice_number'], metrics_df['novapro_model_latency'], marker='o', color='yellow', label='Novapro')\n",
    "ax2.plot(metrics_df['invoice_number'], metrics_df['novalite_model_latency'], marker='o', color='green', label='Novalite')\n",
    "ax2.plot(metrics_df['invoice_number'], metrics_df['haiku3_model_latency'], marker='o', color='blue', label='haiku3')\n",
    "\n",
    "\n",
    "ax2.axhline(y=metrics_df['sonnet3_model_latency'].mean(), color='lightcoral', linestyle='--', label='sonnet3 Mean') \n",
    "ax2.axhline(y=metrics_df['novapro_model_latency'].mean(), color='yellow', linestyle='--', label='Nova pro Mean')\n",
    "ax2.axhline(y=metrics_df['novalite_model_latency'].mean(), color='lightgreen', linestyle='--', label='Nova lite Mean') \n",
    "ax2.axhline(y=metrics_df['haiku3_model_latency'].mean(), color='blue', linestyle='--', label='haiku3 Mean') \n",
    "\n",
    "\n",
    "ax2.set_xlabel('Invoice Number')\n",
    "ax2.set_ylabel('Model Latency (ms)')\n",
    "ax2.set_title('Model Latency Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "ax2.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b4983-c8a6-4b68-a4fb-0eb71f831fbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3023bf-48cd-46ea-83dd-db3763c22b90",
   "metadata": {},
   "source": [
    "### Delete s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d28513-75c5-4e52-85f9-f3df3944a182",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket.objects.all().delete()\n",
    "\n",
    "# Delete the bucket\n",
    "response = s3_client.delete_bucket(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc8c55-e4ab-4af9-8280-6329ba707c16",
   "metadata": {},
   "source": [
    "### Delete Glue Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb517cc-3684-49a3-855c-fbb60cc8bd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = glue_client.delete_crawler(Name=crawler_name)\n",
    "print(f\"Deleting crawler: {crawler_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c7ea2-a62f-47e5-a615-98ac07ef2734",
   "metadata": {},
   "source": [
    "### Delete AWS Glue Data Catalog tables and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0331f-c12e-4b52-bcf9-18569f482b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_tables_in_database(database_name):\n",
    "    # Get a list of all tables in the database\n",
    "    tables = glue_client.get_tables(DatabaseName=database_name)['TableList']\n",
    "\n",
    "    # Iterate over the tables and delete each one\n",
    "    for table in tables:\n",
    "        table_name = table['Name']\n",
    "        glue_client.delete_table(DatabaseName=database_name, Name=table_name)\n",
    "        print(f\"Deleting table: {table_name}\")\n",
    "\n",
    "\n",
    "delete_tables_in_database(database_name)\n",
    "response = glue_client.delete_database(Name=database_name)\n",
    "print(f\"Deleting database: {database_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a00f9-48f4-44c7-9122-a86d8e87819a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
