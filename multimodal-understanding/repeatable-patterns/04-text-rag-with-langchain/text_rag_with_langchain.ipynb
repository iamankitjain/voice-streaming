{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Retrieval Augmented Generation (RAG) with LangChain\n",
    "\n",
    "In this notebook we will cover how to use LangChain retrievers to do Text RAG with LangChain. In the documents folder we have a number of scientific papers that we will store in an in memory DB and use in our RAG chain.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_community faiss_cpu pypdf \"langchain_aws>=0.2.9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process our PDFs\n",
    "\n",
    "We will use the PyPDFDirectorLoader to load all of the documents in the folder. Then, take advantage of the RecursiveCharacterTextSplitter to chunk the documents. This process decomposes large files into chunks that can be fed through our embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def _pdf_to_chunks(doc_path: str):\n",
    "    loader = PyPDFDirectoryLoader(doc_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Vector Store\n",
    "\n",
    "Now that we have the document chunks we can embed them using Bedrock Embeddings (the default embeddings model is Titan Text Embeddings) and store them in a FAISS in memory vectory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "text_chunks = _pdf_to_chunks(\"./media/\")\n",
    "vectorstore = FAISS.from_documents(documents=text_chunks, embedding=BedrockEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RAG Prompt\n",
    "\n",
    "We will now create a prompt that will take the user query and the retrieved context to be sent to the model for the final generation. The provided model instructions show best practices to minimize hallucinations and to have citations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "In this session, the model has access to search results and a user's question, your job is to answer the user's question using only information from the search results.\n",
    "\n",
    "Model Instructions:\n",
    "- You should provide concise answer to simple questions when the answer is directly contained in search results, but when comes to yes/no question, provide some details.\n",
    "- In case the question requires multi-hop reasoning, you should find relevant information from search results and summarize the answer based on relevant information with logical reasoning.\n",
    "- If the search results do not contain information that can answer the question, please state that you could not find an exact answer to the question, and if search results are completely irrelevant, say that you could not find an exact answer, then summarize search results.\n",
    "- Remember to add a citation to the end of your response using markers like [1], [2], [3], etc for the corresponding passage supports the response. \n",
    "- The citations should be able to be rendered in markdown and each citation should be rendered on a new line\n",
    "- DO NOT USE INFORMATION THAT IS NOT IN SEARCH RESULTS!\n",
    "\n",
    "{question} \n",
    "Resource: {context}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Formatting\n",
    "\n",
    "We create a helper function that will take the context from the retriever and format it in a way that provides both the source information and content for the model to reference when doing the final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_docs(docs):\n",
    "    return [{\"source_metadata\": i.metadata, \"source_content\": i.page_content} for i in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the RAG Chain\n",
    "\n",
    "Finally we construct our RAG chain and can invoke the full retriever workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=\"us.amazon.nova-lite-v1:0\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | _format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\n",
    "    rag_chain.invoke(\n",
    "        \"What are the benefits of using multi-agent workflows for translating literary texts. Site your sources\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi-dev-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
